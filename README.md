# LLM-Eval

LLM-Eval is a project designed to evaluate Language Models (LLMs) using various benchmarks. This tool provides a comprehensive way to assess the performance of LLMs across different tasks and datasets.

## Usage

To execute the project, follow these steps:

1. Ensure you have Python 3 installed on your system.

2. Clone this repository to your local machine:
git clone https://github.com/leticiacnavarro/llm-eval.git

3. Run the benchmark script by providing the path to a configuration file:

```
python3 llm-eval/benchmark.py path/to/your/configuration.yaml 
```
Replace `path/to/your/configuration.yaml` with the path to your YAML configuration file.

## Configuration

The configuration file (`configuration.yaml`) allows you to customize the evaluation process. You can specify the LLM to be evaluated, the benchmarks to run, and other parameters relevant to the evaluation.